{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e48e0561-2eeb-4ec6-ab67-87c4ec3bdb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tariq\\anaconda3\\envs\\llms_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import transformers\n",
    "transformers.set_seed(42)\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79415ef0-9ff2-4406-8177-25af5127dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.login(anonymous=\"allow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a5c412-0996-4603-8995-1c9e9752057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"roneneldan/TinyStories-33M\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc749b4-9dc8-4ca9-af35-675aafcb810c",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43d24066-4290-41ba-a53c-ef8e5400bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('MohamedRashad/characters_backstories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3036d5f0-7443-4aeb-ad7b-9f7406663e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Generate Backstory based on following information\\nCharacter Name: Dewin \\nCharacter Race: Halfling\\nCharacter Class: Sorcerer bard\\n\\nOutput:\\n',\n",
       " 'target': 'Dewin thought he was a wizard, but it turned out it was the draconic blood in his veins that brought him eldritch power.  Music classes in wizarding college taught him yet another use for his power, and when he was expelled he took up adventuring'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8997ddbd-637c-4e7a-8b92-f5a13c005dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[\"train\"].train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "010762f9-5475-48d6-9278-4991bc1f4a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(example):\n",
    "    merged = example[\"text\"] + \" \" + example[\"target\"]\n",
    "    batch = tokenizer(merged, padding='max_length', truncation=True, max_length=128)\n",
    "    batch[\"labels\"] = batch[\"input_ids\"].copy()\n",
    "    return batch\n",
    "\n",
    "tokenized_datasets = data.map(tokenize_function, remove_columns=[\"text\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fee8108-676f-4f49-951b-ba82219b24bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Backstory based on following information\n",
      "Character Name: Mr. Gale\n",
      "Character Race: Half-orc\n",
      "Character Class: Cleric\n",
      "\n",
      "Output:\n",
      " Growing up the only half-orc in a small rural town was rough. His mother didn't survive childbirth and so was raised in a church in a high mountain pass, his attention was always drawn by airships passing through, and dreams of an escape. Leaving to strike out on his own as early as he could he made a living for most of his life as an airship sailor, and occasionally a pirate. A single storm visits him throughout his life, marking every major\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_datasets[\"train\"][900]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd23bac-960c-4fab-9416-74b39d98f0ee",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b85ffceb-5fb1-4424-8197-45910dceaaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a77e1aa-63e0-450e-906d-96f85ce68b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = wandb.init(project='language_model_finetuning', job_type=\"finetuning\", anonymous=\"allow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4dc3f6e-7207-46b8-8177-afafe93b3335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tariq\\anaconda3\\envs\\llms_env\\lib\\site-packages\\transformers\\training_args.py:1331: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-characters-backstories\",\n",
    "    report_to=\"wandb\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    no_cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04e72205-59d5-46cf-9f49-e92e8f528e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5fc08e7-eb46-4d8c-a2e2-317771bf6152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='233' max='233' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [233/233 15:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.812700</td>\n",
       "      <td>3.356489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=233, training_loss=3.746427840940942, metrics={'train_runtime': 933.7233, 'train_samples_per_second': 1.989, 'train_steps_per_second': 0.25, 'total_flos': 40423258718208.0, 'train_loss': 3.746427840940942, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb0668cf-82b1-4e44-a90f-40c0fb4cdb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "prefix = \"Generate Backstory based on following information Character Name: \"\n",
    "\n",
    "prompts = [\n",
    "    \"Frogger Character Race: Aarakocra Character Class: Ranger Output: \",\n",
    "    \"Smarty Character Race: Aasimar Character Class: Cleric Output: \",\n",
    "    \"Volcano Character Race: Android Character Class: Paladin Output: \",\n",
    "]\n",
    "\n",
    "table = wandb.Table(columns=[\"prompt\", \"generation\"])\n",
    "\n",
    "for prompt in prompts:\n",
    "    input_ids = tokenizer.encode(prefix + prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, do_sample=True, max_new_tokens=50, top_p=0.3)\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    table.add_data(prefix + prompt, output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05a893ef-909f-419f-82b9-2e5e695a0e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.log({'tiny_generations': table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "109711b8-3ac3-4c6d-962c-413c31092842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f96d1-d660-42a6-80c8-ff6ccaf2bd95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
